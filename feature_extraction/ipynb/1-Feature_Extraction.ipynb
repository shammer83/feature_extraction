{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Assumptions:\n",
    "- data are represented by a fixed number of features which can be binary, categorical or continuous\n",
    "- finding a good data representation is very domain specific and related to available measurements\n",
    "- human expertise, which is often required to convert “raw” data into a set of useful features, can be complemented by automatic feature construction and feature selection methods\n",
    "\n",
    "We will refer to the combined application of *feature construction* and *feature selection* as **feature extraction**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Construction\n",
    "\n",
    "Sometimes called *feature generation*. \n",
    "\n",
    "Feature construction is a preprocessing step in our data pipeline.\n",
    "\n",
    "### Types of Feature Construction\n",
    "\n",
    "##### standardization and normalization\n",
    "\n",
    "See [2-Standardization_and_Normalization.ipynb](2-Standardization_and_Normalization.ipynb) and [3-Scaling.ipynb](3-Scaling.ipynb).\n",
    "\n",
    "##### feature discretization\n",
    "\n",
    "Trivially, you could imagine \"binning\" continuous data, for example, given the feature `age` creating the new feature `is_minor` where `age < 18`.\n",
    "\n",
    "Scikit-learn also includes:\n",
    "\n",
    "- `preprocessing.Binarizer([threshold, copy])`\tBinarize data (set feature values to 0 or 1) according to a threshold\n",
    "- `preprocessing.LabelBinarizer([neg_label, …])`\tBinarize labels in a one-vs-all fashion\n",
    "- `preprocessing.LabelEncoder`\tEncode labels with value between 0 and n_classes-1.\n",
    "- `preprocessing.MultiLabelBinarizer([classes, …])`\tTransform between iterable of iterables and a multilabel format\n",
    "- `preprocessing.OneHotEncoder([n_values, …])`\tEncode categorical integer features using a one-hot aka one-of-K scheme.\n",
    "\n",
    "     \n",
    "##### linear and non-linear space embedding\n",
    "   \n",
    "We have started to look at one technique, Principal Component Analysis.\n",
    "\n",
    "You might explore others in the sklearn package. Have a look at [Comparison_of_Manifold_Learning_Methods.ipynb](Comparison_of_Manifold_Learning_Methods.ipynb)\n",
    "     \n",
    "#####  non-linear expansions\n",
    "   \n",
    "See [4-Simple_Polynomial_Expansion.ipynb](4-Simple_Polynomial_Expansion.ipynb). Later in the course we will talk about kernels and basis functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimensionality\n",
    "\n",
    "Some methods do not alter the space dimensionality (e.g. signal enhancement, normalization, standardization), while others enlarge it (non-linear expansions, feature discretization), reduce it (space embedding methods) or can act in either direction (extraction of local features).   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature construction can largely condition the success of any subsequent statistics or machine learning endeavor. \n",
    "\n",
    "In particular, one should beware of not losing information at the feature construction stage. \n",
    "\n",
    "It may be a good idea to add the raw features to the preprocessed data or at least to compare the performances obtained with either representation. \n",
    " \n",
    "Adding features seems reasonable but it comes at a price: it may increase the dimensionality of the patterns and thereby immerses the relevant information into a sea of possibly irrelevant, noisy or redundant features. \n",
    "\n",
    "How do we know when a feature is relevant or informative? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "select relevant and informative features\n",
    "\n",
    "general data reduction, to limit storage requirements and increase algorithm speed\n",
    "\n",
    "feature set reduction, to save resources in the next round of data collection or during utilization\n",
    "\n",
    "performance improvement, to gain in predictive accuracy\n",
    "\n",
    "data understanding, to gain knowledge about the process that generated the data or simply visualize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Relevant and Informative Features\n",
    "\n",
    "- individual feature relevance\n",
    "   \n",
    "- relevant features that are individually irrelevant\n",
    "   - a helpful feature may be irrelevant by itself\n",
    "   - two individually irrelevant features may be relevant in combination\n",
    "   \n",
    "   See [5-Individual_Feature_Relevance.ipynb](5-Individual_Feature_Relevance.ipynb).\n",
    "   \n",
    "- forward and backward procedures\n",
    "   - recursive feature elimination \n",
    "\n",
    "- redundant features\n",
    "   - eliminate noisy features\n",
    "   - correlation does not imply redundancy\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
